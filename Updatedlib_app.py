# -*- coding: utf-8 -*-
"""Untitled79.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UcLWcxUGxSeevk7T-DPxashyJ4-y8fUQ
"""

#Importing required libraries
import streamlit as st
import requests
import matplotlib.pyplot as plt
from PIL import Image

#Function to classify text and to predict toxicity
def classify_text(text, threshold, url):
    input_data = {"text": text, "ai_score_threshold": threshold}
    response = requests.post(url, json=input_data)
    if response.status_code == 200:
        return response.json()
    else:
        return None

#Defining the ngrok public URL. This is generated by FAST API which was written in Amazon Sagemaker
url =  "https://0190-13-53-64-97.ngrok-free.app/verify_and_check_bias"

#Loading the banner image from the URL
banner_url = "https://github.com/Griffender/Fake-berry/raw/main/Banner.png"
banner_image = Image.open(requests.get(banner_url, stream=True).raw)

#Display the banner image as a header
st.image(banner_image, use_column_width=True)

#Streamlit app
st.title("Fake Berry")
st.write("Craft Your Query: Unmask Bias, Confirm Fairness and validate Authenticity")

# Creating columns for layout
left_col, right_col = st.columns(2)

# Input text box to capture user's input
input_text = left_col.text_area("Input Text", height=200)
ai_score_threshold = left_col.slider("AI Score Threshold", 0.0, 1.0, 0.5)

if left_col.button("Classify"):
    if input_text:
        result = classify_text(input_text, ai_score_threshold, url)
        if result:
            classification = result.get("classification", "N/A")
            left_col.write("**Classification Result:**")
            left_col.write(f"Classification: {classification}")

            if classification == "AI Generated Text":
                toxicity_result = classify_text(input_text, ai_score_threshold, url)
                if toxicity_result:
                    probability_of_toxicity = toxicity_result.get("probability_of_toxicity", 0.0)
                    prediction = toxicity_result.get("prediction", "N/A")

                    left_col.write(f"Prediction: {prediction}")

                    # Plot the circular progress chart for toxicity score
                    fig, ax = plt.subplots()
                    ax.pie([probability_of_toxicity, 1 - probability_of_toxicity],
                           startangle=90, colors=['#FF6F61', '#E0E0E0'],
                           wedgeprops={'width': 0.3})
                    ax.text(0, 0, f"{int(probability_of_toxicity * 100)}%",
                            ha='center', va='center', fontsize=20, color='#FF6F61')
                    ax.set_aspect('equal')

                    # Display the chart in the right column
                    with right_col:
                        st.pyplot(fig)
                else:
                    left_col.error("Error: Unable to classify the text for toxicity. Please try again later.")
        else:
            left_col.error("Error: Unable to classify the text. Please try again later.")
    else:
        left_col.warning("Please enter some text to classify.")