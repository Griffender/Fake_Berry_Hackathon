{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "# Function for basic text preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/webtext.test.csv')\n",
        "\n",
        "# Preprocess the text data\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Handle missing values\n",
        "data.dropna(subset=['text', 'ended'], inplace=True)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(data['text'], data['ended'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)\n",
        "\n",
        "# Tokenize and encode the training and validation data\n",
        "def tokenize_encode(texts, tokenizer, max_length=512):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "train_encodings = tokenize_encode(X_train.tolist(), tokenizer)\n",
        "val_encodings = tokenize_encode(X_val.tolist(), tokenizer)\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(y_train.values))\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(y_val.values))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Fine-tuning the model\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            labels = labels.float().unsqueeze(1)  # BCEWithLogitsLoss expects targets to be float and of shape [batch_size, 1]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}')\n",
        "        evaluate(model, val_loader, criterion)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            labels = labels.float().unsqueeze(1)  # BCEWithLogitsLoss expects targets to be float and of shape [batch_size, 1]\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            total_val_loss += loss.item()\n",
        "            val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_preds = np.array(val_preds)\n",
        "    val_preds = (val_preds > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
        "    model.train()\n",
        "\n",
        "# Fine-tune the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion)\n",
        "\n",
        "# After training, evaluate on the validation set again\n",
        "model.eval()\n",
        "val_preds = []\n",
        "val_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        labels = labels.float().unsqueeze(1)  # BCEWithLogitsLoss expects targets to be float and of shape [batch_size, 1]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "val_preds = np.array(val_preds)\n",
        "val_preds = (val_preds > 0.5).astype(int)\n",
        "accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(f'Final Validation Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW0L3CMLa_Vz",
        "outputId": "68410296-b1d4-4344-e0a4-d731dc51874d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.4291\n",
            "Validation Loss: 0.3576, Validation Accuracy: 0.7910\n",
            "Epoch 2, Training Loss: 0.3396\n",
            "Validation Loss: 0.3220, Validation Accuracy: 0.8270\n",
            "Epoch 3, Training Loss: 0.3123\n",
            "Validation Loss: 0.3071, Validation Accuracy: 0.8360\n",
            "Final Validation Accuracy: 0.8360\n"
          ]
        }
      ]
    }
  ]
}